{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5189a9ce",
   "metadata": {},
   "source": [
    "## An Empirical Study on the Effectiveness of Transferring Software Performance Prediction Machine Learning Models Between Compile-Time Configurations via Linear Transformation\n",
    "\n",
    "### By Harry Spiers, supervised by Dr. Tao Chen\n",
    "\n",
    "Contained within this notebook is the code to run the experiments laid out in the project report. The cells are laid out in the following format:\n",
    "\n",
    "1. Initialisation cell - Contains code to import all required modules, initialise results tables, and select datasets\n",
    "1. Research question 1 cell\n",
    "1. Research question 2 cell\n",
    "1. Research question 3 cell\n",
    "\n",
    "To run the experiments, simply run the initialisation cell first and then run the research question cells in any order using the run button (see below). Output for the experiments will be written to `<project-root>/results`\n",
    "\n",
    "<img src=\"https://i.imgur.com/Hlv9pNb.png\" align=\"left\"><br><br>\n",
    "\n",
    "Note - If the code won't run, then ensure that you have installed the virtual environment correctly. You should see a little .venv string like this one in the top right corner of the screen:\n",
    "\n",
    "<img src=\"https://i.imgur.com/oI0FRz6.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54762f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import constants\n",
    "from data import Dataset, split_transfer_dataset, get_random_datasets\n",
    "from learner import PredictorLearner, TransferLearner\n",
    "from analysis import *\n",
    "\n",
    "# define training set sizes to be used in experiments\n",
    "TRAINING_SET_SIZES = [0.2, 0.4, 0.6, 0.8]\n",
    "SUBJECT_SYSTEMS = constants.SUBJECT_SYSTEMS\n",
    "REPETITIONS = constants.EXPERIMENT_REPS\n",
    "\n",
    "print('Initialising results tables')\n",
    "# initialise results columns\n",
    "rq1_results_fields = constants.RESULTS_DATAFRAME_COLUMN_NAMES[0]\n",
    "rq1_results = pd.DataFrame(columns=rq1_results_fields)\n",
    "rq2_results_fields = constants.RESULTS_DATAFRAME_COLUMN_NAMES[1]\n",
    "rq2_results = pd.DataFrame(columns=rq2_results_fields)\n",
    "rq3_results_fields = constants.RESULTS_DATAFRAME_COLUMN_NAMES[2]\n",
    "rq3_results = pd.DataFrame(columns=rq3_results_fields)\n",
    "print('Results tabled initialised\\n')\n",
    "\n",
    "print('Selecting random datasets')\n",
    "# randomly select a source and target dataset for each subject system\n",
    "datasets = get_random_datasets(reproducibility_mode=True)\n",
    "print('Datasets selected')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f6bdf",
   "metadata": {},
   "source": [
    "## Research Question 1\n",
    "\n",
    "### How much accuracy is lost when transferring a performance prediction model between compile-time configurations via linear transformation compared to training a new model?<br/>\n",
    "\n",
    "**Null hypothesis – It will be more accurate to train a new model for each compile-time configuration**\n",
    "\n",
    "**Alternative hypothesis – It will be more accurate to train a train a transfer model for each compile-time configuration**<br/><br/>\n",
    "\n",
    "This can be thought of as the main question that we wish to answer with this project. We’d like to know how the accuracy compares if you make use of transfer learning between compile-time configurations rather than learning a whole new model. This is important to know as, if there is a big loss in accuracy, then the transfer learning approach may be infeasible.\n",
    "\n",
    "For this research question, we record the following measurements:\n",
    "\n",
    "| Variable name             \t| Measurement                                                             \t|\n",
    "|---------------------------\t|-------------------------------------------------------------------------\t|\n",
    "| mape_accuracy_pred_no_cv  \t| MAPE accuracy of predictor approach without hyperparameter optimisation \t|\n",
    "| mape_accuracy_trans_no_cv \t| MAPE accuracy of transfer approach without hyperparameter optimisation  \t|\n",
    "| mape_accuracy_pred_cv     \t| MAPE accuracy of predictor approach with hyperparameter optimisation    \t|\n",
    "| mape_accuracy_trans_cv    \t| MAPE accuracy of transfer approach with hyperparameter optimisation     \t|\n",
    "| mse_accuracy_pred_no_cv   \t| MSE accuracy of predictor approach without hyperparameter optimisation  \t|\n",
    "| mse_accuracy_trans_no_cv  \t| MSE accuracy of transfer approach without hyperparameter optimisation   \t|\n",
    "| mse_accuracy_pred_cv      \t| MSE accuracy of predictor approach with hyperparameter optimisation     \t|\n",
    "| mse_accuracy_trans_cv     \t| MSE accuracy of transfer approach with hyperparameter optimisation      \t|\n",
    "\n",
    "We calculate the p-value and effect size between `mape_accuracy_pred_cv` and `mape_accuracy_trans_cv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d8ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RQ1\n",
    "\n",
    "predictor = PredictorLearner()\n",
    "transferrer = TransferLearner()\n",
    "\n",
    "for subject_system in SUBJECT_SYSTEMS:\n",
    "    print('***********************************\\nSubject system:', subject_system)\n",
    "    \n",
    "    # delete the results of previous subject system's test before entering next subject system's results\n",
    "    mape_accuracy_pred_no_cv = []\n",
    "    mape_accuracy_trans_no_cv = []\n",
    "    mape_accuracy_pred_cv = []\n",
    "    mape_accuracy_trans_cv = []\n",
    "    mse_accuracy_pred_no_cv = []\n",
    "    mse_accuracy_trans_no_cv = []\n",
    "    mse_accuracy_pred_cv = []\n",
    "    mse_accuracy_trans_cv = []\n",
    "\n",
    "    # get the randomly selected target and source datasets for the current subject system\n",
    "    subject_system_datasets = datasets[subject_system]\n",
    "\n",
    "    for rep in range(REPETITIONS):\n",
    "        print('Experiment repetition', rep+1)\n",
    "        # grab a target and source dataset for this experiment repetition\n",
    "        src_dataset, tgt_dataset = subject_system_datasets[rep]\n",
    "        print('Source dataset path:', src_dataset.get_csv_path())\n",
    "        print('Target dataset path:', tgt_dataset.get_csv_path(),'\\n')\n",
    "\n",
    "        # get optimised predictor model using hyperparameter optimisation\n",
    "        X_train, X_validate, y_train, y_validate = tgt_dataset.get_split_dataset()\n",
    "        optimised_model = predictor.get_optimal_params(X_validate, y_validate)\n",
    "\n",
    "        # get accuracy of optimised model for predictor learner\n",
    "        predictor.fit(X_train, y_train, premade_model=optimised_model)\n",
    "        mape_accuracy_pred_cv.append(predictor.get_error(X_train, y_train, measure='mape'))\n",
    "        mse_accuracy_pred_cv.append(predictor.get_error(X_train, y_train, measure='mse'))\n",
    "\n",
    "        # get accuracy of non-optimised model for predictor learner\n",
    "        predictor.fit(X_train, y_train)\n",
    "        mape_accuracy_pred_no_cv.append(predictor.get_error(X_train, y_train, measure='mape'))\n",
    "        mse_accuracy_pred_no_cv.append(predictor.get_error(X_train, y_train, measure='mse'))\n",
    "\n",
    "\n",
    "        # get optimised transfer model using hyperparameter optimisation\n",
    "        X_train, X_validate, y_train, y_validate = split_transfer_dataset(src_dataset, tgt_dataset)\n",
    "        optimised_model = transferrer.get_optimal_params(X_validate, y_validate)\n",
    "\n",
    "        # get accuracy of optimised model for transfer learner\n",
    "        transferrer.fit(X_train, y_train, premade_model=optimised_model)\n",
    "        mape_accuracy_trans_cv.append(transferrer.get_error(X_train, y_train, measure='mape'))\n",
    "        mse_accuracy_trans_cv.append(predictor.get_error(X_train, y_train, measure='mse'))\n",
    "        make_transfer_model_scatter_plot(transferrer.get_model(), \n",
    "                                         X_train, \n",
    "                                         y_train, \n",
    "                                         1, \n",
    "                                         mape_accuracy_trans_cv[-1], \n",
    "                                         rep+1, \n",
    "                                         subject_system)\n",
    "\n",
    "        # get accuracy of non-optimised model for transfer learner\n",
    "        transferrer.fit(X_train, y_train)\n",
    "        mape_accuracy_trans_no_cv.append(transferrer.get_error(X_train, y_train, measure='mape'))\n",
    "        mse_accuracy_trans_no_cv.append(predictor.get_error(X_train, y_train, measure='mse'))\n",
    "\n",
    "\n",
    "    rq1_results['mse_accuracy_tgt_no_cv'] = mse_accuracy_pred_no_cv\n",
    "    rq1_results['mape_accuracy_tgt_no_cv'] = mape_accuracy_pred_no_cv\n",
    "    rq1_results['mse_accuracy_tgt_cv'] = mse_accuracy_pred_cv\n",
    "    rq1_results['mape_accuracy_tgt_cv'] = mape_accuracy_pred_cv\n",
    "    rq1_results['mse_accuracy_trans_no_cv'] = mse_accuracy_trans_no_cv\n",
    "    rq1_results['mape_accuracy_trans_no_cv'] = mape_accuracy_trans_no_cv\n",
    "    rq1_results['mse_accuracy_trans_cv'] = mse_accuracy_trans_cv\n",
    "    rq1_results['mape_accuracy_trans_cv'] = mape_accuracy_trans_cv\n",
    "    \n",
    "    p_value = [get_wilcoxon_p_value(mape_accuracy_pred_cv, mape_accuracy_trans_cv)]\n",
    "    cliffs_delta = [get_cliffs_delta(mape_accuracy_pred_cv, mape_accuracy_trans_cv)]\n",
    "    print('Wilcoxon p value:', p_value)\n",
    "    print('Cliff\\'s delta:', cliffs_delta, '\\n')\n",
    "\n",
    "    save_results(rq1_results, subject_system.lower(), p_value, cliffs_delta, 1)\n",
    "\n",
    "    # reset dataframe after results are saved to csv\n",
    "    rq1_results = pd.DataFrame(columns=rq1_results.columns)\n",
    "    \n",
    "make_box_plots(1)\n",
    "write_mean_min_max(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aec607c",
   "metadata": {},
   "source": [
    "## Research Question 2\n",
    "\n",
    "### How does the size of the training dataset impact the accuracy of our transfer model?<br/>\n",
    "\n",
    "**Null hypothesis – The size of the training dataset will have no effect on the accuracy of the transfer model**\n",
    "\n",
    "**Alternative hypothesis – The size of the dataset will have an effect on the transfer model’s accuracy**<br/><br/>\n",
    "\n",
    "Research has shown that the accuracy of performance prediction models generally increases as the size of the training set increases [1, 4]. Valov et. al [14] also found that their transfer model for transferring performance prediction models between hardware environments’ accuracy increased with the training set size. As such, I believe that it is worth testing how much my transfer model’s accuracy depends on the training set size. It is also important to test how much data is required to attain an acceptable level of accuracy (if such accuracy is possible).\n",
    "\n",
    "For this research question, we record the following measurements:\n",
    "\n",
    "| Variable name             \t| Measurement                                                            \t|\n",
    "|---------------------------\t|------------------------------------------------------------------------\t|\n",
    "| mape_accuracy_pred_20pct  \t| MAPE accuracy of predictor approach with 20% training set size         \t|\n",
    "| mape_accuracy_pred_40pct  \t| MAPE accuracy of predictor approach with 40% training set size \t        |\n",
    "| mape_accuracy_pred_60pct  \t| MAPE accuracy of predictor approach with 60% training set size   \t        |\n",
    "| mape_accuracy_pred_80pct  \t| MAPE accuracy of predictor approach with 80% training set size    \t    |\n",
    "| mape_accuracy_trans_20pct \t| MAPE accuracy of transfer approach with 20% training set size \t        |\n",
    "| mape_accuracy_trans_40pct \t| MAPE accuracy of transfer approach with 40% training set size  \t        |\n",
    "| mape_accuracy_trans_60pct \t| MAPE accuracy of transfer approach with 60% training set size    \t        |\n",
    "| mape_accuracy_trans_80pct \t| MAPE accuracy of transfer approach with 80% training set size     \t    |\n",
    "| mse_accuracy_pred_20pct   \t| MSE accuracy of predictor approach with 20% training set size             |\n",
    "| mse_accuracy_pred_40pct   \t| MSE accuracy of predictor approach with 40% training set size             |\n",
    "| mse_accuracy_pred_60pct   \t| MSE accuracy of predictor approach with 60% training set size             |\n",
    "| mse_accuracy_pred_80pct   \t| MSE accuracy of predictor approach with 80% training set size             |\n",
    "| mse_accuracy_trans_20pct  \t| MSE accuracy of transfer approach with 20% training set size              |\n",
    "| mse_accuracy_trans_40pct  \t| MSE accuracy of transfer approach with 40% training set size              |\n",
    "| mse_accuracy_trans_60pct  \t| MSE accuracy of transfer approach with 60% training set size              |\n",
    "| mse_accuracy_trans_80pct  \t| MSE accuracy of transfer approach with 80% training set size              |\n",
    "\n",
    "We calculate the p-value and effect size between `mape_accuracy_trans_80pct` and `mape_accuracy_trans_20pct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b4dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RQ2\n",
    "\n",
    "transferrer = TransferLearner()\n",
    "predictor = PredictorLearner()\n",
    "\n",
    "for subject_system in SUBJECT_SYSTEMS:\n",
    "    print('***********************************\\nSubject system:', subject_system)    \n",
    "    # delete the results of previous subject system's test before entering next subject system's results\n",
    "    mape_accuracy_pred_20pct = []\n",
    "    mape_accuracy_pred_40pct = []\n",
    "    mape_accuracy_pred_60pct = []\n",
    "    mape_accuracy_pred_80pct = []\n",
    "    mape_accuracy_trans_20pct = []\n",
    "    mape_accuracy_trans_40pct = []\n",
    "    mape_accuracy_trans_60pct = []\n",
    "    mape_accuracy_trans_80pct = []\n",
    "    mse_accuracy_pred_20pct = []\n",
    "    mse_accuracy_pred_40pct = []\n",
    "    mse_accuracy_pred_60pct = []\n",
    "    mse_accuracy_pred_80pct = []\n",
    "    mse_accuracy_trans_20pct = []\n",
    "    mse_accuracy_trans_40pct = []\n",
    "    mse_accuracy_trans_60pct = []\n",
    "    mse_accuracy_trans_80pct = []\n",
    "\n",
    "    # get the randomly selected target and source datasets for the current subject system\n",
    "    subject_system_datasets = datasets[subject_system]\n",
    "\n",
    "    for rep in range(REPETITIONS):\n",
    "        print('Experiment repetition', rep+1)\n",
    "        # grab a target and source dataset for this experiment repetition\n",
    "        src_dataset, tgt_dataset = subject_system_datasets[rep]\n",
    "        print('Source dataset path:', src_dataset.get_csv_path())\n",
    "        print('Target dataset path:', tgt_dataset.get_csv_path(),'\\n')\n",
    "        \n",
    "        X_train, X_validate, y_train, y_validate = tgt_dataset.get_split_dataset()\n",
    "        optimised_pred_model = predictor.get_optimal_params(X_validate, y_validate)\n",
    "        \n",
    "        X_train, X_validate, y_train, y_validate = split_transfer_dataset(src_dataset, tgt_dataset)\n",
    "        optimised_trans_model = transferrer.get_optimal_params(X_validate, y_validate)\n",
    "\n",
    "        for train_size in TRAINING_SET_SIZES:\n",
    "            X_train, X_validate, y_train, y_validate = tgt_dataset.get_split_dataset(train_size=train_size)\n",
    "\n",
    "            # get accuracy of predictor model for current training set size\n",
    "            predictor.fit(X_train, y_train, premade_model=optimised_pred_model)\n",
    "            mape_accuracy_pred = predictor.get_error(X_train, y_train, measure='mape')\n",
    "            mse_accuracy_pred = predictor.get_error(X_train, y_train, measure='mse')\n",
    "\n",
    "\n",
    "            X_train, X_validate, y_train, y_validate = split_transfer_dataset(src_dataset, \n",
    "                                                      tgt_dataset, \n",
    "                                                      train_size=train_size)\n",
    "            # get accuracy of transfer model for each training set size\n",
    "            transferrer.fit(X_train, y_train, premade_model=optimised_trans_model)\n",
    "            mape_accuracy_trans = transferrer.get_error(X_train, y_train, measure='mape')\n",
    "            mse_accuracy_trans = transferrer.get_error(X_train, y_train, measure='mse')\n",
    "\n",
    "\n",
    "            # record accuracy in appropriate results column\n",
    "            if train_size == 0.2:\n",
    "                mape_accuracy_pred_20pct.append(mape_accuracy_pred)\n",
    "                mape_accuracy_trans_20pct.append(mape_accuracy_trans)\n",
    "                mse_accuracy_pred_20pct.append(mse_accuracy_pred)\n",
    "                mse_accuracy_trans_20pct.append(mse_accuracy_trans)\n",
    "                make_transfer_model_scatter_plot(transferrer.get_model(), \n",
    "                                 X_train, \n",
    "                                 y_train, \n",
    "                                 2, \n",
    "                                 mape_accuracy_trans_20pct[-1], \n",
    "                                 rep+1, \n",
    "                                 subject_system,\n",
    "                                 dataset_size=train_size*100)\n",
    "            elif train_size == 0.4:\n",
    "                mape_accuracy_pred_40pct.append(mape_accuracy_pred)\n",
    "                mape_accuracy_trans_40pct.append(mape_accuracy_trans)\n",
    "                mse_accuracy_pred_40pct.append(mse_accuracy_pred)\n",
    "                mse_accuracy_trans_40pct.append(mse_accuracy_trans)\n",
    "                make_transfer_model_scatter_plot(transferrer.get_model(), \n",
    "                                 X_train, \n",
    "                                 y_train, \n",
    "                                 2, \n",
    "                                 mape_accuracy_trans_40pct[-1], \n",
    "                                 rep+1, \n",
    "                                 subject_system,\n",
    "                                 dataset_size=train_size*100)\n",
    "            elif train_size == 0.6:\n",
    "                mape_accuracy_pred_60pct.append(mape_accuracy_pred)\n",
    "                mape_accuracy_trans_60pct.append(mape_accuracy_trans)\n",
    "                mse_accuracy_pred_60pct.append(mse_accuracy_pred)\n",
    "                mse_accuracy_trans_60pct.append(mse_accuracy_trans)\n",
    "                make_transfer_model_scatter_plot(transferrer.get_model(), \n",
    "                                 X_train, \n",
    "                                 y_train, \n",
    "                                 2, \n",
    "                                 mape_accuracy_trans_60pct[-1], \n",
    "                                 rep+1, \n",
    "                                 subject_system,\n",
    "                                 dataset_size=train_size*100)\n",
    "            elif train_size == 0.8:\n",
    "                mape_accuracy_pred_80pct.append(mape_accuracy_pred)\n",
    "                mape_accuracy_trans_80pct.append(mape_accuracy_trans)\n",
    "                mse_accuracy_pred_80pct.append(mse_accuracy_pred)\n",
    "                mse_accuracy_trans_80pct.append(mse_accuracy_trans)\n",
    "                make_transfer_model_scatter_plot(transferrer.get_model(), \n",
    "                                 X_train, \n",
    "                                 y_train, \n",
    "                                 2, \n",
    "                                 mape_accuracy_trans_80pct[-1], \n",
    "                                 rep+1, \n",
    "                                 subject_system,\n",
    "                                 dataset_size=train_size*100)\n",
    "\n",
    "\n",
    "    rq2_results['mape_accuracy_pred_20pct'] = mape_accuracy_pred_20pct\n",
    "    rq2_results['mape_accuracy_pred_40pct'] = mape_accuracy_pred_40pct\n",
    "    rq2_results['mape_accuracy_pred_60pct'] = mape_accuracy_pred_60pct\n",
    "    rq2_results['mape_accuracy_pred_80pct'] = mape_accuracy_pred_80pct\n",
    "    rq2_results['mse_accuracy_pred_20pct'] = mse_accuracy_pred_20pct\n",
    "    rq2_results['mse_accuracy_pred_40pct'] = mse_accuracy_pred_40pct\n",
    "    rq2_results['mse_accuracy_pred_60pct'] = mse_accuracy_pred_60pct\n",
    "    rq2_results['mse_accuracy_pred_80pct'] = mse_accuracy_pred_80pct\n",
    "    rq2_results['mape_accuracy_trans_20pct'] = mape_accuracy_trans_20pct\n",
    "    rq2_results['mape_accuracy_trans_40pct'] = mape_accuracy_trans_40pct\n",
    "    rq2_results['mape_accuracy_trans_60pct'] = mape_accuracy_trans_60pct\n",
    "    rq2_results['mape_accuracy_trans_80pct'] = mape_accuracy_trans_80pct\n",
    "    rq2_results['mse_accuracy_trans_20pct'] = mse_accuracy_trans_20pct\n",
    "    rq2_results['mse_accuracy_trans_40pct'] = mse_accuracy_trans_40pct\n",
    "    rq2_results['mse_accuracy_trans_60pct'] = mse_accuracy_trans_60pct\n",
    "    rq2_results['mse_accuracy_trans_80pct'] = mse_accuracy_trans_80pct\n",
    "    \n",
    "    p_value = [get_wilcoxon_p_value(mape_accuracy_trans_80pct, mape_accuracy_trans_20pct)]\n",
    "    cliffs_delta = [get_cliffs_delta(mape_accuracy_trans_80pct, mape_accuracy_trans_20pct)]\n",
    "    print('Wilcoxon p value:', p_value)\n",
    "    print('Cliff\\'s delta:', cliffs_delta, '\\n')\n",
    "    \n",
    "    save_results(rq2_results, subject_system.lower(), p_value, cliffs_delta, 2)\n",
    "    \n",
    "    # reset dataframe after results are saved to csv\n",
    "    rq2_results = pd.DataFrame(columns=rq2_results.columns)\n",
    "    \n",
    "make_box_plots(2)\n",
    "write_mean_min_max(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b4f65b",
   "metadata": {},
   "source": [
    "## Research Question 3\n",
    "\n",
    "### How does the training time for a transfer model compare to training a new predictor model for each compile-time configuration?<br/>\n",
    "\n",
    "**Null hypothesis – The training time for training a new predictor model for each compile-time configuration will be faster than training a transfer model for each compile-time configuration**\n",
    "\n",
    "**Alternative hypothesis – The training time for training a transfer model for each compile-time configuration will be faster than training a new predictor model for each compile-time configuration**<br/><br/>\n",
    "\n",
    "This question will test how feasible our approach is in terms of training time. For the most part, training a performance prediction model doesn’t take very long, usually under 100ms. This is because the size of the required input data to achieve good accuracy is small, usually around 15-100 measured configurations depending on the quality of the training data. However, I believe that it is worth testing how long it takes compared to learning a predictor new model each time because if a user needs to test lots of compile-time configurations, then it could be more efficient to learn a transfer model rather than a predictor model. By answering this question, readers can get an idea for whether it’d be more beneficial for them to make use of transfer learning, or just learn a new model for each compile-time configuration.\n",
    "\n",
    "For this research question, we record the following measurements (in milliseconds):\n",
    "\n",
    "| Variable name             \t| Measurement                                                            \t|\n",
    "|---------------------------\t|------------------------------------------------------------------------\t|\n",
    "| training_time_pred_20pct_no_cv  \t| Training time of predictor approach with 20% training set size without hyperparameter optimisation         \t|\n",
    "| training_time_pred_40pct_no_cv  \t| Training time of predictor approach with 40% training set size without hyperparameter optimisation \t        |\n",
    "| training_time_pred_60pct_no_cv  \t| Training time of predictor approach with 60% training set size without hyperparameter optimisation   \t        |\n",
    "| training_time_pred_80pct_no_cv  \t| Training time of predictor approach with 80% training set size without hyperparameter optimisation    \t    |\n",
    "| training_time_pred_20pct_cv \t| Training time of predictor approach with 20% training set size with hyperparameter optimisation \t        |\n",
    "| training_time_pred_40pct_cv \t| Training time of predictor approach with 40% training set size with hyperparameter optimisation  \t        |\n",
    "| training_time_pred_60pct_cv \t| Training time of predictor approach with 60% training set size with hyperparameter optimisation    \t        |\n",
    "| training_time_pred_80pct_cv \t| Training time of predictor approach with 80% training set size with hyperparameter optimisation     \t    |\n",
    "| training_time_trans_20pct_no_cv   \t| Training time of transfer approach with 20% training set size without hyperparameter optimisation             |\n",
    "| training_time_trans_40pct_no_cv   \t| Training time of transfer approach with 40% training set size without hyperparameter optimisation             |\n",
    "| training_time_trans_60pct_no_cv   \t| Training time of transfer approach with 60% training set size without hyperparameter optimisation             |\n",
    "| training_time_trans_80pct_no_cv   \t| Training time of transfer approach with 80% training set size without hyperparameter optimisation             |\n",
    "| training_time_trans_20pct_cv  \t| Training time of transfer approach with 20% training set size with hyperparameter optimisation              |\n",
    "| training_time_trans_40pct_cv  \t| Training time of transfer approach with 40% training set size with hyperparameter optimisation              |\n",
    "| training_time_trans_60pct_cv  \t| Training time of transfer approach with 60% training set size with hyperparameter optimisation              |\n",
    "| training_time_trans_80pct_cv  \t| Training time of transfer approach with 80% training set size with hyperparameter optimisation              |\n",
    "\n",
    "We calculate the p-value and effect size between `mape_accuracy_trans_80pct` and `mape_accuracy_trans_20pct`, and `training_time_pred_80pct_cv` and `training_time_trans_80pct_cv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dea000",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RQ3\n",
    "\n",
    "transferrer = TransferLearner()\n",
    "predictor = PredictorLearner()\n",
    "\n",
    "for subject_system in SUBJECT_SYSTEMS:\n",
    "    print('***********************************\\nSubject system:', subject_system)\n",
    "    # delete the results of previous subject system's test before entering next subject system's results\n",
    "    training_time_pred_20pct_no_cv = []\n",
    "    training_time_pred_40pct_no_cv = []\n",
    "    training_time_pred_60pct_no_cv = []\n",
    "    training_time_pred_80pct_no_cv = []\n",
    "    training_time_pred_20pct_cv = []\n",
    "    training_time_pred_40pct_cv = []\n",
    "    training_time_pred_60pct_cv = []\n",
    "    training_time_pred_80pct_cv = []\n",
    "    training_time_trans_20pct_no_cv = []\n",
    "    training_time_trans_40pct_no_cv = []\n",
    "    training_time_trans_60pct_no_cv = []\n",
    "    training_time_trans_80pct_no_cv = []\n",
    "    training_time_trans_20pct_cv = []\n",
    "    training_time_trans_40pct_cv = []\n",
    "    training_time_trans_60pct_cv = []\n",
    "    training_time_trans_80pct_cv = []\n",
    "\n",
    "    # get the randomly selected target and source datasets for the current subject system\n",
    "    subject_system_datasets = datasets[subject_system]\n",
    "\n",
    "    for rep in range(REPETITIONS):\n",
    "        print('Experiment repetition', rep+1)\n",
    "        # grab a target and source dataset for this experiment repetition\n",
    "        src_dataset, tgt_dataset = subject_system_datasets[rep]\n",
    "        print('Source dataset path:', src_dataset.get_csv_path())\n",
    "        print('Target dataset path:', tgt_dataset.get_csv_path(),'\\n')\n",
    "        \n",
    "        X_train, X_validate, y_train, y_validate = tgt_dataset.get_split_dataset()\n",
    "        optimised_pred_model = predictor.get_optimal_params(X_validate, y_validate)\n",
    "        X_train, X_validate, y_train, y_validate = split_transfer_dataset(src_dataset, tgt_dataset)\n",
    "        optimised_trans_model = transferrer.get_optimal_params(X_validate, y_validate)\n",
    "        \n",
    "\n",
    "        for train_size in TRAINING_SET_SIZES:\n",
    "\n",
    "            X_train, X_validate, y_train, y_validate = tgt_dataset.get_split_dataset(train_size=train_size)\n",
    "            # get optimised predictor model using hyperparameter optimisation\n",
    "            predictor.fit(X_train, y_train, premade_model=optimised_pred_model)\n",
    "\n",
    "            # gather results\n",
    "            training_time_pred_no_cv = predictor.get_training_time()\n",
    "            training_time_pred_cv = predictor.get_training_time(include_optimisation_time=True)\n",
    "\n",
    "\n",
    "            X_train, X_validate, y_train, y_validate = split_transfer_dataset(src_dataset, tgt_dataset, train_size=train_size)\n",
    "            # get optimised transfer model using hyperparameter optimisation\n",
    "            transferrer.fit(X_train, y_train, premade_model=optimised_trans_model)\n",
    "\n",
    "            # gather results\n",
    "            training_time_trans_no_cv = transferrer.get_training_time()\n",
    "            training_time_trans_cv = transferrer.get_training_time(include_optimisation_time=True)\n",
    "\n",
    "\n",
    "            # record accuracy in appropriate results column\n",
    "            if train_size == 0.2:\n",
    "                training_time_pred_20pct_no_cv.append(training_time_pred_no_cv)\n",
    "                training_time_pred_20pct_cv.append(training_time_pred_cv)\n",
    "                training_time_trans_20pct_no_cv.append(training_time_trans_no_cv)\n",
    "                training_time_trans_20pct_cv.append(training_time_trans_cv)\n",
    "            elif train_size == 0.4:\n",
    "                training_time_pred_40pct_no_cv.append(training_time_pred_no_cv)\n",
    "                training_time_pred_40pct_cv.append(training_time_pred_cv)\n",
    "                training_time_trans_40pct_no_cv.append(training_time_trans_no_cv)\n",
    "                training_time_trans_40pct_cv.append(training_time_trans_cv)\n",
    "            elif train_size == 0.6:\n",
    "                training_time_pred_60pct_no_cv.append(training_time_pred_no_cv)\n",
    "                training_time_pred_60pct_cv.append(training_time_pred_cv)\n",
    "                training_time_trans_60pct_no_cv.append(training_time_trans_no_cv)\n",
    "                training_time_trans_60pct_cv.append(training_time_trans_cv)\n",
    "            elif train_size == 0.8:\n",
    "                training_time_pred_80pct_no_cv.append(training_time_pred_no_cv)\n",
    "                training_time_pred_80pct_cv.append(training_time_pred_cv)\n",
    "                training_time_trans_80pct_no_cv.append(training_time_trans_no_cv)\n",
    "                training_time_trans_80pct_cv.append(training_time_trans_cv)\n",
    "\n",
    "    rq3_results['training_time_pred_20pct_no_cv'] = training_time_pred_20pct_no_cv\n",
    "    rq3_results['training_time_pred_40pct_no_cv'] = training_time_pred_40pct_no_cv\n",
    "    rq3_results['training_time_pred_60pct_no_cv'] = training_time_pred_60pct_no_cv\n",
    "    rq3_results['training_time_pred_80pct_no_cv'] = training_time_pred_80pct_no_cv\n",
    "    rq3_results['training_time_pred_20pct_cv'] = training_time_pred_20pct_cv\n",
    "    rq3_results['training_time_pred_40pct_cv'] = training_time_pred_40pct_cv\n",
    "    rq3_results['training_time_pred_60pct_cv'] = training_time_pred_60pct_cv\n",
    "    rq3_results['training_time_pred_80pct_cv'] = training_time_pred_80pct_cv\n",
    "    rq3_results['training_time_trans_20pct_no_cv'] = training_time_trans_20pct_no_cv\n",
    "    rq3_results['training_time_trans_40pct_no_cv'] = training_time_trans_40pct_no_cv\n",
    "    rq3_results['training_time_trans_60pct_no_cv'] = training_time_trans_60pct_no_cv\n",
    "    rq3_results['training_time_trans_80pct_no_cv'] = training_time_trans_80pct_no_cv\n",
    "    rq3_results['training_time_trans_20pct_cv'] = training_time_trans_20pct_cv\n",
    "    rq3_results['training_time_trans_40pct_cv'] = training_time_trans_40pct_cv\n",
    "    rq3_results['training_time_trans_60pct_cv'] = training_time_trans_60pct_cv\n",
    "    rq3_results['training_time_trans_80pct_cv'] = training_time_trans_80pct_cv\n",
    "    \n",
    "    p_value_no_cv = get_wilcoxon_p_value(training_time_pred_80pct_no_cv, training_time_trans_80pct_no_cv)\n",
    "    p_value_cv = get_wilcoxon_p_value(training_time_pred_80pct_cv, training_time_trans_80pct_cv)\n",
    "    cliffs_delta_no_cv = get_cliffs_delta(training_time_pred_80pct_no_cv, training_time_trans_80pct_no_cv)\n",
    "    cliffs_delta_cv = get_cliffs_delta(training_time_pred_80pct_cv, training_time_trans_80pct_cv)\n",
    "    p_values = [p_value_cv, p_value_no_cv]\n",
    "    cliffs_deltas = [cliffs_delta_cv, cliffs_delta_no_cv]\n",
    "    \n",
    "    print('Wilcoxon p value for no CV:', p_value_no_cv)\n",
    "    print('Wilcoxon p value for CV:', p_value_cv)\n",
    "    print('Cliff\\'s delta for no CV:', cliffs_delta_no_cv)\n",
    "    print('Cliff\\'s delta for CV:', cliffs_delta_cv, '\\n')\n",
    "\n",
    "    save_results(rq3_results, subject_system.lower(), p_values, cliffs_deltas, 3)\n",
    "\n",
    "    # reset dataframe after results are saved to csv\n",
    "    rq3_results = pd.DataFrame(columns=rq3_results.columns)\n",
    "    \n",
    "make_box_plots(3)\n",
    "write_mean_min_max(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
